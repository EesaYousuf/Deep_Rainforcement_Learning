{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e141534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Tuple, Optional, Iterable\n",
    "import math\n",
    "import random\n",
    "import statistics\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27cb55e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"\n",
    "    Abstract Environment.\n",
    "    - state: representation of the current situation (S_t)\n",
    "    - action: decision taken by the agent (A_t)\n",
    "    - reward: scalar feedback from env (R_{t+1})\n",
    "    - transition function: P(s', r | s, a) -- implemented via step()\n",
    "    \"\"\"\n",
    "    def reset(self) -> Any:\n",
    "        \"\"\"Start a new episode: returns initial state s0.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def step(self, action: Any) -> Tuple[Any, float, bool, Dict]:\n",
    "        \"\"\"\n",
    "        Take action in current state.\n",
    "        Returns: (next_state, reward, done, info)\n",
    "        - done indicates episode termination\n",
    "        - this embodies the transition function and reward function\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def action_space(self) -> Iterable[Any]:\n",
    "    \n",
    "        \"\"\"Available actions in the current state (or a fixed set).\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Abstract Agent mapping states to actions.\n",
    "    - policy π(a|s) implemented by act(state)\n",
    "    - may update internal knowledge by observe(...)\n",
    "    \"\"\"\n",
    "    def act(self, state: Any) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def observe(self, s: Any, a: Any, r: float, s_next: Any, done: bool):\n",
    "        \"\"\"Learning update after each transition.\"\"\"\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31b27fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# Section 2: A tiny GridWorld Environment\n",
    "# ======================================\n",
    "\n",
    "@dataclass\n",
    "class GridWorld(Environment):\n",
    "    \"\"\"\n",
    "    GridWorld:\n",
    "    - Grid of size H x W with walls, terminal states, rewards\n",
    "    - Deterministic transitions (for simplicity)\n",
    "    - Episode ends when terminal state is reached or max_steps exceeded\n",
    "    \"\"\"\n",
    "    height: int = 4\n",
    "    width: int = 4\n",
    "    start: Tuple[int, int] = (0, 0)\n",
    "    terminals: Dict[Tuple[int, int], float] = field(default_factory=lambda: {(3, 3): +1.0, (3, 0): -1.0})\n",
    "    walls: set[Tuple[int, int]] = field(default_factory=set)\n",
    "    step_reward: float = -0.04  # small penalty to encourage shortest path\n",
    "    max_steps: int = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7660d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2489153160.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    _state: Tuple[int, int] = field(init=False, default=(0, 0))\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Internal runtime state (not part of definition)\n",
    "#       _state: Tuple[int, int] = field(init=False, default=(0, 0))\n",
    "      _steps: int = field(init=False, default=0)\n",
    "\n",
    "      def __post_init__(self):\n",
    "        self._state = self.start\n",
    "        self._steps = 0\n",
    "\n",
    "      def reset(self) -> Tuple[int, int]:\n",
    "        self._state = self.start\n",
    "        self._steps = 0\n",
    "        return self._state\n",
    "\n",
    "    def in_bounds(self, pos: Tuple[int, int]) -> bool:\n",
    "        r, c = pos\n",
    "        return 0 <= r < self.height and 0 <= c < self.width\n",
    "\n",
    "    def is_terminal(self, pos: Tuple[int, int]) -> bool:\n",
    "        return pos in self.terminals\n",
    "\n",
    "    def action_space(self) -> List[str]:\n",
    "        return [\"U\", \"D\", \"L\", \"R\"]\n",
    "\n",
    "    def _move(self, s: Tuple[int, int], a: str) -> Tuple[int, int]:\n",
    "        r, c = s\n",
    "        if a == \"U\":\n",
    "            nxt = (r - 1, c)\n",
    "        elif a == \"D\":\n",
    "            nxt = (r + 1, c)\n",
    "        elif a == \"L\":\n",
    "            nxt = (r, c - 1)\n",
    "        elif a == \"R\":\n",
    "            nxt = (r, c + 1)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown action\")\n",
    "\n",
    "        # Transition function T(s,a)->s' with constraints\n",
    "        if not self.in_bounds(nxt) or nxt in self.walls:\n",
    "            nxt = s  # bump into wall -> stay\n",
    "\n",
    "        return nxt\n",
    "\n",
    "    def step(self, action: str) -> Tuple[Tuple[int, int], float, bool, Dict]:\n",
    "        assert action in self.action_space(), f\"Invalid action {action}\"\n",
    "        if self.is_terminal(self._state):\n",
    "            # If already terminal, keep returning terminal\n",
    "            r = self.terminals[self._state]\n",
    "            return self._state, r, True, {\"reason\": \"already_terminal\"}\n",
    "\n",
    "        s = self._state\n",
    "        s_next = self._move(s, action)\n",
    "        self._steps += 1\n",
    "\n",
    "        # Reward function R(s,a,s')\n",
    "        r = self.step_reward\n",
    "        done = False\n",
    "        if self.is_terminal(s_next):\n",
    "            r = self.terminals[s_next]\n",
    "            done = True\n",
    "        elif self._steps >= self.max_steps:\n",
    "            done = True  # safety cap on episode length\n",
    "\n",
    "        self._state = s_next\n",
    "        return s_next, r, done, {}\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# Section 3: A simple Q-learning Agent\n",
    "# ===================================\n",
    "\n",
    "@dataclass\n",
    "class QLearningAgent(Agent):\n",
    "    \"\"\"\n",
    "    Tabular Q-learning:\n",
    "    Q(s,a) <- Q(s,a) + α [r + γ max_a' Q(s',a') - Q(s,a)]\n",
    "    \"\"\"\n",
    "    actions: List[Any]\n",
    "    alpha: float = 0.5          # learning rate\n",
    "    gamma: float = 0.99         # discount factor γ (present value of future rewards)\n",
    "    epsilon: float = 0.1        # ε-greedy exploration\n",
    "    q: Dict[Any, Dict[Any, float]] = field(default_factory=dict)\n",
    "    rng: random.Random = field(default_factory=random.Random)\n",
    "\n",
    "    def _ensure_state(self, s: Any):\n",
    "        if s not in self.q:\n",
    "            self.q[s] = {a: 0.0 for a in self.actions}\n",
    "\n",
    "    def act(self, state: Any) -> Any:\n",
    "        self._ensure_state(state)\n",
    "        # ε-greedy policy π(a|s)\n",
    "        if self.rng.random() < self.epsilon:\n",
    "            return self.rng.choice(self.actions)\n",
    "        # greedy action with tie-breaking\n",
    "        a_vals = self.q[state]\n",
    "        max_q = max(a_vals.values())\n",
    "        best = [a for a, v in a_vals.items() if v == max_q]\n",
    "        return self.rng.choice(best)\n",
    "\n",
    "    def observe(self, s: Any, a: Any, r: float, s_next: Any, done: bool):\n",
    "        self._ensure_state(s)\n",
    "        self._ensure_state(s_next)\n",
    "        max_next = 0.0 if done else max(self.q[s_next].values())\n",
    "        td_target = r + self.gamma * max_next\n",
    "        td_error = td_target - self.q[s][a]\n",
    "        self.q[s][a] += self.alpha * td_error\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# Section 4: Running an Episode (loop driver)\n",
    "# =========================================\n",
    "\n",
    "def run_episode(env: Environment, agent: Agent, render: bool = False, max_steps: Optional[int] = None) -> float:\n",
    "    \"\"\"\n",
    "    One full episode:\n",
    "      s0 = env.reset()\n",
    "      repeat:\n",
    "        a = agent.act(s_t)\n",
    "        s_{t+1}, r_{t+1}, done = env.step(a)\n",
    "        agent.observe(...)\n",
    "      until done\n",
    "    Returns total discounted return G_0 = sum_t γ^t R_{t+1} (computed here for illustration).\n",
    "    \"\"\"\n",
    "    s = env.reset()\n",
    "    total_return = 0.0\n",
    "    gamma = getattr(agent, \"gamma\", 1.0)  # discount from agent if available\n",
    "    g_pow = 1.0\n",
    "    steps = 0\n",
    "\n",
    "    while True:\n",
    "        a = agent.act(s)\n",
    "        s_next, r, done, info = env.step(a)\n",
    "        agent.observe(s, a, r, s_next, done)\n",
    "\n",
    "        total_return += g_pow * r\n",
    "        g_pow *= gamma\n",
    "        s = s_next\n",
    "        steps += 1\n",
    "\n",
    "        if render:\n",
    "            print(f\"step={steps:02d} s={s} a={a} r={r:+.2f} done={done}\")\n",
    "\n",
    "        if done or (max_steps is not None and steps >= max_steps):\n",
    "            break\n",
    "\n",
    "    return total_return\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Section 5: Multi-Armed Bandits (stateless RL subproblem)\n",
    "# ======================================================\n",
    "\n",
    "class BanditEnv:\n",
    "    \"\"\"\n",
    "    Stationary K-armed bandit with Bernoulli rewards.\n",
    "    No state transitions (single-state MDP), only actions (arms) and rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, probs: List[float], rng: Optional[random.Random] = None):\n",
    "        self.probs = probs\n",
    "        self.k = len(probs)\n",
    "        self.rng = rng or random.Random()\n",
    "\n",
    "    def pull(self, arm: int) -> int:\n",
    "        \"\"\"Return 1 with probability p_arm, else 0.\"\"\"\n",
    "        return 1 if self.rng.random() < self.probs[arm] else 0\n",
    "\n",
    "\n",
    "class EpsilonGreedyBanditAgent:\n",
    "    \"\"\"\n",
    "    ε-greedy for bandits:\n",
    "    - Estimates: Q[a] = average reward of arm a so far\n",
    "    - With probability ε explore, else exploit best Q\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k: int, epsilon: float = 0.1, rng: Optional[random.Random] = None):\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.counts = [0] * k\n",
    "        self.values = [0.0] * k\n",
    "        self.rng = rng or random.Random()\n",
    "\n",
    "    def act(self) -> int:\n",
    "        if self.rng.random() < self.epsilon:\n",
    "            return self.rng.randrange(self.k)\n",
    "        max_q = max(self.values)\n",
    "        best = [i for i, v in enumerate(self.values) if v == max_q]\n",
    "        return self.rng.choice(best)\n",
    "\n",
    "    def observe(self, arm: int, reward: float):\n",
    "        self.counts[arm] += 1\n",
    "        # incremental mean update\n",
    "        n = self.counts[arm]\n",
    "        self.values[arm] += (reward - self.values[arm]) / n\n",
    "\n",
    "\n",
    "class UCBBanditAgent:\n",
    "    \"\"\"\n",
    "    UCB1 (Upper Confidence Bound):\n",
    "      Select arm i maximizing: Q[i] + sqrt( (2 ln t) / N[i] )\n",
    "      Encourages exploration based on uncertainty\n",
    "    \"\"\"\n",
    "    def __init__(self, k: int):\n",
    "        self.k = k\n",
    "        self.counts = [0] * k\n",
    "        self.values = [0.0] * k\n",
    "        self.t = 0\n",
    "\n",
    "    def act(self) -> int:\n",
    "        self.t += 1\n",
    "        # play each arm once initially\n",
    "        for i in range(self.k):\n",
    "            if self.counts[i] == 0:\n",
    "                return i\n",
    "        ucb_scores = [\n",
    "            self.values[i] + math.sqrt(2.0 * math.log(self.t) / self.counts[i])\n",
    "            for i in range(self.k)\n",
    "        ]\n",
    "        max_u = max(ucb_scores)\n",
    "        best = [i for i, u in enumerate(ucb_scores) if u == max_u]\n",
    "        return random.choice(best)\n",
    "\n",
    "    def observe(self, arm: int, reward: float):\n",
    "        self.counts[arm] += 1\n",
    "        n = self.counts[arm]\n",
    "        self.values[arm] += (reward - self.values[arm]) / n\n",
    "\n",
    "\n",
    "class ThompsonBernoulliBanditAgent:\n",
    "    \"\"\"\n",
    "    Thompson Sampling for Bernoulli bandits:\n",
    "      Maintain Beta(α, β) posterior per arm; sample θ ~ Beta(α, β) and play argmax θ\n",
    "    \"\"\"\n",
    "    def __init__(self, k: int, prior_alpha: float = 1.0, prior_beta: float = 1.0, rng: Optional[random.Random] = None):\n",
    "        self.k = k\n",
    "        self.alpha = [prior_alpha] * k\n",
    "        self.beta = [prior_beta] * k\n",
    "        self.rng = rng or random.Random()\n",
    "\n",
    "    def act(self) -> int:\n",
    "        samples = [random.betavariate(self.alpha[i], self.beta[i]) for i in range(self.k)]\n",
    "        max_s = max(samples)\n",
    "        best = [i for i, s in enumerate(samples) if s == max_s]\n",
    "        return self.rng.choice(best)\n",
    "\n",
    "    def observe(self, arm: int, reward: int):\n",
    "        # reward must be 0/1 for Bernoulli\n",
    "        if reward == 1:\n",
    "            self.alpha[arm] += 1\n",
    "        else:\n",
    "            self.beta[arm] += 1\n",
    "\n",
    "\n",
    "def run_bandit(env: BanditEnv, agent, steps: int = 1000) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Simulate bandit interaction and collect stats.\n",
    "    Returns dict with total reward, average reward, arm counts, estimated values.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    rewards = []\n",
    "    arm_counts = [0] * env.k\n",
    "\n",
    "    for t in range(steps):\n",
    "        arm = agent.act()\n",
    "        r = env.pull(arm)\n",
    "        total += r\n",
    "        rewards.append(r)\n",
    "        arm_counts[arm] += 1\n",
    "        agent.observe(arm, r)\n",
    "\n",
    "    return {\n",
    "        \"total_reward\": total,\n",
    "        \"avg_reward\": sum(rewards) / len(rewards),\n",
    "        \"arm_counts\": arm_counts,\n",
    "        \"estimates\": getattr(agent, \"values\", None),\n",
    "        \"alpha\": getattr(agent, \"alpha\", None),\n",
    "        \"beta\": getattr(agent, \"beta\", None),\n",
    "    }\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Section 6: Demo runners (safe to run as __main__)\n",
    "# =====================================================\n",
    "\n",
    "def demo_gridworld_q_learning(episodes: int = 500):\n",
    "    print(\"=== GridWorld + Q-learning (episodic) ===\")\n",
    "    env = GridWorld(height=4, width=4, start=(0, 0),\n",
    "                    terminals={(3, 3): +1.0, (3, 0): -1.0}, step_reward=-0.04)\n",
    "    agent = QLearningAgent(actions=env.action_space(), alpha=0.5, gamma=0.99, epsilon=0.1)\n",
    "    returns = []\n",
    "    for ep in range(1, episodes + 1):\n",
    "        G = run_episode(env, agent, render=False)\n",
    "        returns.append(G)\n",
    "        if ep % (episodes // 5) == 0:\n",
    "            print(f\"Episode {ep:4d} | running avg return (last 50): {statistics.mean(returns[-50:]):+.3f}\")\n",
    "\n",
    "    # Show learned Q-values for a few states\n",
    "    print(\"\\nSample Q-values:\")\n",
    "    for s in [(0,0), (0,1), (1,1), (2,2), (3,2)]:\n",
    "        if s in agent.q:\n",
    "            print(s, agent.q[s])\n",
    "\n",
    "def demo_bandits():\n",
    "    print(\"\\n=== Bandits: ε-greedy vs UCB1 vs Thompson (Bernoulli) ===\")\n",
    "    true_ps = [0.1, 0.2, 0.5, 0.3]\n",
    "    env = BanditEnv(true_ps)\n",
    "\n",
    "    steps = 5000\n",
    "    eg = EpsilonGreedyBanditAgent(k=len(true_ps), epsilon=0.1)\n",
    "    ucb = UCBBanditAgent(k=len(true_ps))\n",
    "    ts = ThompsonBernoulliBanditAgent(k=len(true_ps))\n",
    "\n",
    "    res_eg = run_bandit(env, eg, steps)\n",
    "    res_ucb = run_bandit(env, ucb, steps)\n",
    "    res_ts = run_bandit(env, ts, steps)\n",
    "\n",
    "    def pretty(name, res):\n",
    "        print(f\"{name:10s} | total={res['total_reward']:4d}  avg={res['avg_reward']:.3f}  pulls={res['arm_counts']}\")\n",
    "\n",
    "    pretty(\"epsilon-g\", res_eg)\n",
    "    pretty(\"ucb1\",      res_ucb)\n",
    "    pretty(\"thompson\",  res_ts)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run both demos\n",
    "    demo_gridworld_q_learning(episodes=600)\n",
    "    demo_bandits()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
